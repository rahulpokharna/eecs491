{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and computation\n",
    "###### The actual file is included in the zip file, this notebook is to include the functions and derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation function is derived similarly to the textbook, where we calculate the posterior distribution with the current data and then send that to be used in the calculation of the maximization.\n",
    "\n",
    "For the expectation function, we will need to calculate and evaluate the posterior at the current step, to determine the probabilities of each \"class\". This is essentially $ p_{n,k} = p(c_k| x^{(n)}, \\theta_{1:K}) $ where K is the number of classes in the mixture, $\\\\theta$ is the parameters for all of the classes, and $x$ refers to the nth data point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expectation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The maximization function then takes the posterior distribution, the current $\\theta$ estimation, and of course the data. Using these, it computes the next step, by maximizing the gradient by seeting the log normal distribution to zero and then evaluati"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
