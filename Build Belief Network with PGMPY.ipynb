{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Belief Network with PGMPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains two simple examples to show how to use *PGMPY* package, which is a *Python* library for *Probabilistic Graphical Models*. You can access all related resources through its [official website](http://pgmpy.org/). You can install the package as simple as:\n",
    "\n",
    "```bash\n",
    ">> pip3 install pgmpy\n",
    "```\n",
    "\n",
    "However, in my system, there is a dependent package named 'wrapt' not installed in this problem. I need install it specificly.\n",
    "\n",
    "```bash\n",
    ">> pip3 install wrapt\n",
    "```\n",
    "\n",
    "Then you can access classes defined in package through keyword **import** in *Python*, like:\n",
    "\n",
    "```python\n",
    "from pgmpy.models import BayesianModel\n",
    "```\n",
    "\n",
    "Which can import classes and methods related to Bayesian models, which is also our major topic here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One : Inspector Clouseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is an example comes from book of Barber (Example 2 in Chapater 1)*\n",
    "\n",
    "Inspector Clouseau arrives at the scene of a crime. The victim lies dead in the room and the inspector quickly ﬁnds the murder weapon, a Knife (K). The Butler (B) and Maid (M) are his main suspects. The inspector has a prior belief of 0.8 that the Butler is the murderer, and a prior belief of 0.2 that the Maid is the murderer. These probabilities are independent in the sense that p(B, M) = p(B)p(M). (It is possible that both the Butler and the Maid murdered the victim or neither). The inspector’s prior criminal knowledge can be formulated mathematically as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(B) & = 0.8 \\\\\n",
    "P(M) & = 0.2 \\\\\n",
    "P(M, B) & = P(M)P(B)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Besides, Clouseau also has estimations that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(K | \\bar{B}, \\bar{M}) & = 0.3 \\\\\n",
    "P(K | \\bar{B}, M) & = 0.2 \\\\\n",
    "P(K | B, \\bar{M}) & = 0.6 \\\\\n",
    "P(K | B, M) & = 0.1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What is the probability that the Butler is the murderer? (Remember that it might be that neither is the murderer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we can get help from **pgmpy** package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel as bysmodel\n",
    "from pgmpy.factors.discrete import TabularCPD as tcpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Up Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **pgmpy**, Bayesian network is specified by variables and connection between them. For building up a functional Bayesian model, you need describe the structure of network by claiming links in the corresponding directed graph, and assign conditional probabilities (or prior) to each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model with specifying connections between random variables\n",
    "model = bysmodel([('B', 'K'), ('M', 'K')]);\n",
    "\n",
    "# define prior with TabularCPD (TABULAR Conditional Probability Distribution) without condition\n",
    "priorB = tcpd(variable='B', variable_card=2, values=[[0.2, 0.8]])\n",
    "priorM = tcpd(variable='M', variable_card=2, values=[[0.8, 0.2]])\n",
    "\n",
    "# define conditional probability of 'K' and 'B' with 'M'\n",
    "cpdK = tcpd(variable='K', variable_card=2, \n",
    "            evidence=['B', 'M'], evidence_card=[2, 2],\n",
    "            values=[[0.7, 0.8, 0.4, 0.9], \n",
    "                    [0.3, 0.2, 0.6, 0.1]]\n",
    "           )\n",
    "\n",
    "# add probabilities to model\n",
    "model.add_cpds(priorB, priorM, cpdK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we built up our model, a simple way to check minor mistakes is function *check_model*. It will tell you whether or not your model fullfilled the axioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check structure and values consistency with Bayesian Model\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also can let it show you conditional probability in tables. Let's compare it with our setup:\n",
    "\n",
    "|           | B = false, M = false | B = false, M = true | B = true, M = false | B = true, M = true |\n",
    "| --------- | -------------------- | ------------------- | ------------------- | ------------------ |\n",
    "| K = false | 0.7 | 0.8 | 0.4 | 0.9 |\n",
    "| K = true  | 0.3 | 0.2 | 0.6 | 0.1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_cpds('K'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination as proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(B|K) & = \\sum_{M} P(B,M|K) \\\\\n",
    "       & = \\sum_{M} \\dfrac{P(B,M,K)}{P(K)} \\\\\n",
    "       & = \\dfrac{P(B) \\sum_{M} P(K|B,M)P(M)}{\\sum_{B} P(B) \\sum_{M} P(K|B,M) P(M)} \\\\\n",
    "       & = \\dfrac{0.8 \\times (0.1 \\times 0.2 + 0.6 \\times 0.8)}{0.8 \\times (0.1 \\times 0.2 + 0.6 \\times 0.8) + 0.2 \\times (0.2 \\times 0.2 + 0.3 \\times 0.8)} \\\\\n",
    "       & \\approx 0.877\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's solve it with **pgmpy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = proc(model)\n",
    "print(infer.query(['B'], evidence={'K' : 1}) ['B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two : Poisson Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Poisson distribution has the following form\n",
    "\n",
    "$$\n",
    "p(n|\\lambda) = \\frac{1}{n!} \\lambda^n e^{-\\lambda}\n",
    "$$\n",
    "\n",
    "where λ > 0 is the rate of the Poisson process, and n is the number of observations per unit of time, n = {1, 2, . . .}. To derive the posterior distribution, we must deﬁne a prior. A natural choice is the gamma distribution distribution, which is a conjugate prior for the Poisson distribution\n",
    "\n",
    "$$\n",
    "\\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n",
    "$$\n",
    "\n",
    "where α is the shape parameter and β is the inverse scale parameter. A special case of the gamma is the exponential, Gamma(α = 1, β)\n",
    "\n",
    "$$\n",
    "\\beta e^{-\\beta \\lambda}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Derive the posterior distribution p(λ|n, T) where T is the total observation time, and λ is speciﬁed in terms of events per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of events $n$ that happened in an unit time follows :\n",
    "$$p(n|\\lambda) = \\dfrac{1}{n!} \\lambda^n e^{-\\lambda}$$\n",
    "\n",
    "Considering the condition that $n$ in a particular time interval $T$, we have :\n",
    "$$p(n|\\lambda,T) = \\dfrac{1}{n!} (\\lambda T)^n e^{-\\lambda T}$$\n",
    "\n",
    "We assign $\\Gamma$ distribution as the **Prior** of $\\ \\lambda$ :\n",
    "$$p(\\lambda|\\alpha,\\beta) = \\dfrac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}$$\n",
    "\n",
    "Utilizing Bayes Rules, we can gradually get $Posterior$ of $\\ \\lambda$ :\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\tp(\\lambda|n,\\alpha,\\beta,T) \n",
    "\t\t& = & \\dfrac{p(n|\\lambda,\\alpha,\\beta,T) p(\\lambda|\\alpha,\\beta,T)}{p(n|\\alpha,\\beta,T)} \\\\\n",
    "\t    & = & \\dfrac{p(n|\\lambda,T)p(\\lambda|\\alpha,\\beta)}{\\int_{\\lambda=0}^{\\inf} p(n|\\lambda,T) p(\\lambda|\\alpha,\\beta) d\\lambda} \\\\\n",
    "\t    & = & \\dfrac{\\frac{1}{n!}(\\lambda T)^n e^{-\\lambda T} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}}{\\int_{\\lambda = 0}^{\\inf} \\frac{1}{n!}(\\lambda T)^n e^{-\\lambda T} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} d\\lambda} \\\\\n",
    "\t    & = & \\dfrac{\\lambda^{(\\alpha+n-1)} e^{-(\\beta+T)\\lambda}}{\\int_{\\lambda=0}^{\\inf} \\lambda^{(\\alpha+n-1)} e^{-(\\beta+T)\\lambda} d\\lambda} \\\\\n",
    "\t    & = & \\dfrac{(\\beta+T)^{(\\alpha+n)}}{\\Gamma(\\alpha+n)} \\lambda^{(\\alpha+n-1)} e^{-(\\beta+T)\\lambda}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "There are two independence used in the deduction : between $\\alpha$, $\\beta$, and $n$; and between $\\lambda$ and $T$. \n",
    "\n",
    "The last step is based on the definition of $\\Gamma()$ function :\n",
    "$$\\Gamma(\\alpha) = \\int_{t=0}^{\\inf} e^{-t} t^{\\alpha-1} dt$$\n",
    "\n",
    "While, $\\int_{t=0}^{\\inf} e^{-\\beta t} t^{\\alpha-1} dt$ can be transformed to $\\int_{\\mu=0}^{\\inf} \\frac{1}{\\beta^\\alpha} e^{-\\mu} \\mu^{\\alpha-1} d\\mu$ by variable replacement $\\mu = \\beta t$. Thus,\n",
    "$$\\int_{t=0}^{\\inf} e^{-\\beta t} t^{\\alpha-1} dt = \\dfrac{\\Gamma(\\alpha)}{\\beta^\\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plots the posterior distribution given the following event times\n",
    "\n",
    "```\n",
    "0.53, 0.65, 0.91, 1.19, 1.30, 1.33, 1.90, 2.01, 2.48\n",
    "```\n",
    "\n",
    "Turn in plots of your belief about λ (i.e. the posterior distribution) given the above data at observation times T = {0, 0.5, 1, 5}. Label your axes, and make sure each plot has a clear title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, **pgmpy** package cannot help use in continuous variable inference in Bayesian models. However, it can be useful in *Markov Random Field* with *Sum-Product* algorithm. Here, we'll draw the plot based on our theoretical result above.\n",
    "\n",
    "At very beginning, let's define a *Gamma* function:\n",
    "\n",
    "$$\n",
    "\\Gamma(x, \\alpha, \\beta, n, T) = \\dfrac{(\\beta+T)^{(\\alpha+n)}}{\\Gamma(\\alpha+n)} x^{(\\alpha+n-1)} e^{-(\\beta+T) x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mygamma(x, alpha, beta, n, T):\n",
    "    a = alpha + n\n",
    "    b = beta + T\n",
    "    return b**a * x**(a-1) * math.e**(-b*x) / math.gamma(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make given data a list for calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0.53, 0.65, 0.91, 1.19, 1.30, 1.33, 1.90, 2.01, 2.48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, setup parameters with our prior $\\alpha = \\beta = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2\n",
    "beta  = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw posterior $p(\\lambda|n, T)$ by counting at each time point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5, 500)\n",
    "for T in range(0,4):\n",
    "    n = len([t for t in data if t <= T])\n",
    "    plt.plot(x, mygamma(x, alpha, beta, n, T))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best estimation by searching for maxium value in posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "T = math.ceil(data[-1])\n",
    "# calculate posterior\n",
    "posteriorDist = list(mygamma(x, alpha, beta, n, T))\n",
    "# find maximum\n",
    "print(\"Best Estimation of λ : \", x[posteriorDist.index(max(posteriorDist))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
